---
title: "p8105 HW6"
author: "Ravi Brenner"
output: 
  github_document:
    df_print: kable
---

```{r setup, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(modelr)
set.seed(1)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%",
  dpi = 300,
  warning = FALSE,
  message = FALSE
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Introduction

This homework will demonstrate the use of linear models in R, including bootstrapping and cross validation.

# Methods

The datasets can be downloaded from the homework assignment on the [course website](<https://p8105.com/homework_6.html>). R packages used include `tidyverse`, `broom`, and `modelr`, as well as the `rnoaa` package to get the weather data.

# Problems

## Problem 1

First, pull weather data for Central Park using the `rnoaa` package. Also clean it up a bit for easier use.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Here we will write a very simple linear model, using tmax as the outcome and tmin as the predictor. We'll write a function to extract the two values we're interested in, $r^2$ and $\log{(\beta_0 * \beta_1)}$ and return them as a tibble.

```{r}
weather_fit <- lm(tmax ~ tmin, data = weather_df)

value_extract <- function(model) {
  r_squared <- model |>
    broom::glance() |>
    select(r.squared) |>
    pull()
  
  beta0 <- model |>
    broom::tidy() |>
    filter(term == "(Intercept)") |>
    select(estimate) |>
    pull()
  
  beta1 <- model |>
    broom::tidy() |>
    filter(term == "tmin") |>
    select(estimate) |>
    pull()
  
  output <- tibble(r_squared = r_squared,
                   log_beta0_beta1 = log(beta0 * beta1))
  
  return(output)
}

value_extract(weather_fit)
```

Here is a plot of this model

```{r}
weather_df |>
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


Now, we can use 5000 bootstrap samples, model the tmax ~ tmin relationship for each, and extract the results for  $r^2$ and $\log{(\beta_0 * \beta_1)}$ for each.

```{r}
boot_results <- weather_df |>
  bootstrap(5000) |>
  mutate(strap = map(strap, as_tibble),
         models = map(strap, \(df) lm(tmax ~ tmin, data = df)),
         results = map(models, value_extract)) |>
  select(.id, results) |>
  unnest(results) 
```

Plotting these distributions

```{r}
boot_results |>
  ggplot(aes(x = r_squared)) +
  geom_density() +
  geom_vline(aes(xintercept=mean(r_squared)),
             linetype="dashed") +
  labs(x = latex2exp::TeX("$r^2$"))
```

$r^2$ Looks fairly symmetrical around 0.91, with a slightly longer tail at the lower end, and a fairly wide area where the density peaks.

```{r}
boot_results |>
  ggplot(aes(x = log_beta0_beta1)) +
  geom_density() + 
  geom_vline(aes(xintercept=mean(log_beta0_beta1)),
             linetype="dashed") +
  labs(x = latex2exp::TeX("$\\log{(\\beta_0 * \\beta_1)}$"))
```

$\log{(\beta_0 * \beta_1)}$ reaches a sharper peak just above 2, but is otherwise also very symmetrical.

We can also produce 95% confidence intervals for these values, using the 0.025 and 0.975 quantiles.

```{r}
boot_results |>
  pivot_longer(cols  = -.id, 
               names_to = "var",
               values_to = "value") |>
  group_by(var) |>
  summarize(boot_est = mean(value),
            boot_ci_ll = quantile(value, 0.025),
            boot_ci_ul = quantile(value, 0.975))
```

## Problem 2

This problem uses homicide data collected by the Washington Post First importing the data, and doing some data cleaning to ensure that we have only valid data. We're only interested in white or black race, so we will also filter based on that.

```{r}
homicide_df <- read_csv("data/homicide-data.csv") |>
  mutate(city_state = str_c(city,", ",state),
         solved = if_else(disposition == "Closed by arrest",TRUE, FALSE),
         victim_age = as.numeric(victim_age)) |>
  filter(!city_state %in% c("Dallas, TX",
                            "Phoenix, AZ",
                            "Kansas City, MO",
                            "Tulsa, AL"),
         victim_race %in% c("White","Black"))
```


Here, we will fit a logistic regression model with resolved vs. unresolved as the outcome and victim age, sex, and race as predictors, focusing only on Baltimore, MD for now.

```{r}
baltimore_fit <- homicide_df |>
  filter(city_state == "Baltimore, MD") |>
  glm(solved ~ victim_age + victim_sex + victim_race, 
                     family = binomial,
                     data = _)
```

Here is a brief Function to get the estimate and 95% CI of the adjusted OR for solving homicides comparing male to female victims, holding all other variables fixed.

```{r}
or_ci <- function(model) {
  broom::tidy(model, conf.int = TRUE) |>
    mutate(or = exp(estimate),
           or_low = exp(conf.low),
           or_high = exp(conf.high)) |>
    filter(term == "victim_sexMale") |>
    select(or,or_low,or_high) 
}

or_ci(baltimore_fit)
```


Now we can run this logistic regression model for each city in the dataset, and get the odds ratio and confidence interval for solving homicides comparing male victims to female victims.

```{r}
city_male_or <- homicide_df |>
  nest(.by = city_state) |>
  mutate(models = map(data, \(df) glm(solved ~ victim_age + victim_sex + victim_race,
                                      family = binomial,
                                      data = df)),
         results = map(models, or_ci)) |>
  select(city_state, results) |>
  unnest(results)
```

We should note that these was a warning calculating the confidence interval for some cities. This may be fine, but it is worth interpreting the results with caution.

Plotting these results:

```{r, fig.asp=0.8}
city_male_or |>
  mutate(city_state = fct_reorder(city_state, or)) |>
  ggplot(aes(y = city_state, x = or)) + 
  geom_point() +
  geom_errorbar(aes(xmin = or_low, xmax = or_high)) + 
  geom_vline(xintercept = 1, linetype = "dashed") + 
  labs(x = "OR",
       y = "City, state") +
  theme(axis.text.y = element_text(size = 7))
```

For cities to the left of 1, the odds of murder being solved are lower for male victims than female victims (odds ratio \< 1). Cities to the right of OR=1, odds of murder being solved are higher for males than females. Based on the 95% CI, there are no cities where the odds for a murder being solves for males is higher than 1 with 95% confidence.

## Problem 3

This problem uses data about the effects of several variables on a child's birthweight. First, we will load and clean the data for regression analysis (primarily converting numeric variables to factors).

```{r}
bwt_df <- read_csv("data/birthweight.csv") |>
  mutate(babysex = factor(babysex, labels = c("male","female")),
         frace = factor(frace, 
                        levels = c(1,2,3,4,8,9),
                        labels = c("White","Black","Asian","Puerto Rican","Other","Unknown")),
         mrace = factor(mrace, 
                        levels = c(1,2,3,4,8),
                        labels = c("White","Black","Asian","Puerto Rican","Other")),
         malform = factor(malform,
                          level = c(0,1),
                          labels = c("absent","present")))
```

Not knowing much about this data, I first want to understand the relationship between each of these variables vs. birthweight. I can accomplish this by plotting them in a grid. To do this I will separate out the numeric and factor variables:

First numeric variables:
```{r, fig.asp=0.8}
bwt_df |>
  select(where(is.numeric)) |>
  pivot_longer(-bwt,
               names_to = "covariate",
               values_to = "value") |>
  ggplot(aes(x = value, y = bwt)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_wrap(~covariate, scales = "free_x")
```

And factor variables
```{r, fig.asp=0.8}
bwt_df |>
  select(bwt, !where(is.numeric)) |>
  pivot_longer(-bwt,
               names_to = "covariate",
               values_to = "value") |>
  ggplot(aes(x = value, y = bwt)) + 
  geom_boxplot() + 
  facet_wrap(~covariate, scales = "free_x")
```

Based on these, I will focus on the numeric variables. Seeing that there is a negative relationship with `smoken` (number of cigarette's smoked), I want to include that while controlling for covariates like `blength`, `gaweeks`, `ppbmi` (since this captures both `mheight` and `ppwt`), and `wtgain`, which should give an estimate for the impact of smoking on birthweight controlling for some more "structural" factors.

My hypothesized model:

```{r}
speculative_fit <- lm(bwt ~ smoken + blength + gaweeks + ppbmi + wtgain,
                      data = bwt_df)

```

Now we can make a plot of residuals vs. fitted values:

```{r}
add_residuals(bwt_df, speculative_fit)  |>
  add_predictions(speculative_fit) |>
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(x = "Fitted values",
       y = "Residuals") 
```

The residuals are generally centered around 0, although there are some outlier values on the high and low end that could be concerning.

We can compare this model with two others.

One using length at birth and gestational age as predictors:

```{r}
len_age_fit <- lm(bwt ~ blength + gaweeks,
                  data = bwt_df)
```

And one using head circumference, length, sex, and all interactions between these:

```{r}
interact_fit <- lm(bwt ~ bhead * blength * babysex,
                   data = bwt_df)
```

Now we can use cross validation (doing multiple training and testing splits) to calculate the prediction error for each of these models. Specifically, we'll calculate the RMSE for each cross-validated model and compare them.

```{r}
cv_df <- 
  crossv_mc(bwt_df, 100) |>
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) 

cv_res_df <- cv_df |>
  mutate(
    spec_mod = map(train, \(x) lm(bwt ~ smoken + blength + gaweeks + ppbmi + wtgain, data = x)),
    len_age_mod = map(train, \(x) lm(bwt ~ blength + gaweeks, data = x)),
    interact_mod = map(train, \(x) lm(bwt ~ bhead * blength * babysex, data = x))
    ) |>
  mutate(
    rmse_spec = map2_dbl(spec_mod, test, rmse),
    rmse_len_age = map2_dbl(len_age_mod, test, rmse),
    rmse_interact = map2_dbl(interact_mod, test, rmse)
  )
```

```{r}
cv_res_df |>
  select(starts_with("rmse")) |>
  pivot_longer(everything(),
               names_to = "model",
               values_to = "rmse",
               names_prefix = "rmse_") |>
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() + 
  labs(x = "",
       y = "Prediction error")
```

Based on these results, it looks like my hypothetical model had slightly lower prediction error than the model based on length and gestational age only, but the model based on the interaction of `bhead`, `blength`, and `babysex` had the lowest prediction error of the three.

# Conclusion
This homework demonstrated several ways to use R for regression modeling, including the use of bootstrapping and cross-validation